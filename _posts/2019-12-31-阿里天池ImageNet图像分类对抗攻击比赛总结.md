---
layout: post
title: 阿里天池 ImageNet 图像分类对抗攻击比赛总结
date: 2019-12-31 23:59
tags:
- comp
- AI
categories: AI
description: 阿里天池ImageNet图像分类对抗攻击比赛总结
---


持续 26 天的阿里天池比赛让我获益匪浅。本来去年我还是一个专业的安全研究人员，但是从今年春天开始就不断探索安全技术和 AI 技术的交叉领域，这种对交叉领域的探索不仅仅保持了对学习的极高的热情，还能够对 AI 的相关领域有着新的认识。

非常开心能够和伙伴一起钻研 26 天的AI对抗技术。按照我的笔记统计浏览过 71 篇论文或者往届的解题报告，通读了 26 篇重要的论文，精读了 9 篇在这次比赛中起关键上分的论文。不得不说虽然我是第一次接触这个方向的比赛，但是我觉得自己非常热爱这个领域！

这段时期基本就是天天熬夜，一醒来就考虑改善算法，下午调参，顺便错过吃饭，相当有规律~

解决这次比赛主要有二种思路，一个是基于模型的，真正有点 AI 意思的方式；另一种是基于规则的，使用简单的 python 程序就可以搞定的方式。

一般来说，这类AI比赛很多时候规则真的都比模型好使，我们使用规则直接取得了第五名的成绩：

![](https://saferman.github.io/assets/img/tianchi_imagenet_adversarial_comp/排行榜.png)

赛后也知道前面的大部分选手都是用规则上分的，后期我们并没有继续改进规则的方式，虽然能上分但是总觉得缺少学习的乐趣，自己还是更想钻研算法，毕竟有希望能在未来发表该领域的论文。

通过模型的方法只取得了第 28 名的成绩。我觉得这次比赛之所以模型表现如此差在于和过去的多个AI对抗比赛不同，这次主办方使用了三个最先进的(state-of-the-art)防御技术，严重降低了模型攻击技术的效果！

有多严重呢，请看我们复现往届类似比赛冠军的算法的成绩（进入前十至少得是 1.0 以上）：

- NIPS *2017 AI 对抗*性攻防竞赛冠军清华大学同学的 M-I-FGSM 算法：得分 0.13，排名倒数
- 2018 CAAD 对抗比赛冠军算法优化的 M-FGSM：得分 0.24，排名倒数
- 阿里上半年 IJCAI 的人脸识别对抗算法，我们改进为适合这次比赛的代码：得分 0.28，排名倒数

这还只是无目标攻击的得分，在有目标攻击中不仅没有成功，还使得无目标攻击的效果下降。赛后也问了出题人，这次比赛的防御缺少比较 diao，他们都没有特别有效的攻击思路。

让我欣慰的是，即使在如此困难的题目约束下，我和我的队友抱着想研读论文改进算法（想之后发 paper）的目的，从多个维度将我们的模型攻击效果逐步提升，下图是从提交记录中绘制的我们模型得分趋势：

![](https://saferman.github.io/assets/img/tianchi_imagenet_adversarial_comp/trend.png)

这次比赛是完全黑盒场景（还有三个防御技术），只能使用完全黑盒攻击的技术（连查询黑盒攻击技术都很难实施），或者使用白盒模型攻击技术但是需要极大地提高迁移能力。下面我总结一下在这次比赛中提升白盒模型攻击技术的 tricks。

### 基础模型的选择

模型融合能够极大地提高白盒攻击的迁移能力，特别是网络结构不同的那些模型。这次比赛我们主要使用 torch 实现的，经过尝试如下四种预训练模型融合的得分最高：

```
resnet_model, vgg19_model, inception_v3_model , inceptionresnetv2_model
```

融合的位置在 logits（softmax前），我的权重参数是都是1/4。之所以选择在 softmax 前融合参考的是这篇论文的实验结论：

> Dong, Yinpeng & Pang, Tianyu & Su, Hang & Zhu, Jun. (2019). Evading Defenses to Transferable Adversarial Examples by Translation-Invariant Attacks. 

AI 对抗领域一个非常有效的防御方式就是讲攻击图片作为训练集给模型训练提高模型对这类攻击的防御能力，比如截止目前为止网络上公开的经过大量 Imagenet 攻击样本预训练的鲁棒性模型如下：

> [干净模型](https://github.com/tensorflow/models/tree/master/research/slim)：resnet_v2_101、inception_v3、inception_resnet_v2
>
> [迭代步数比较少的对抗训练模型](https://github.com/tensorflow/models/tree/master/research/adv_imagenet_models)：AdvInceptionV3、Ens3AdvInceptionV3、Ens4AdvInceptionV3、EnsAdvInceptionResnetV2、InceptionResnetV2
>
> [迭代步数比较多的对抗训练模型](https://github.com/facebookresearch/ImageNet-Adversarial-Training/blob/master/INSTRUCTIONS.md)：ResNet152_PGD、ResNet152_Denoise、ResNeXt101_DenoiseAl

将鲁棒性的防御模型和正常的预训练模型融合作为白盒攻击的模型对提升攻击的迁移能力非常有效（这些预训练的模型都是 tensorflow 版本的）。这次因为时间问题没有使用第三个号称在 128 块 GPU 训练的防御模型。

这次比赛一个不足的点是我们的程序架构最初是我设计的，因为队员都擅长 torch 所以用 torch 完成整个比赛代码，但是现阶段公开的预训练adv模型都是 tf 版本，导致我们比赛期间并没有改写全部的这类模型，这也是赛后才知道我们和前几名的差距所在。

### 数据增强

提升白盒攻击迁移能力的核心就是防止本地的白盒攻击过拟合，数据增强是AI对抗比赛中非有效的提分方案。这次比赛的后期的模型上分主要靠这个。下面列举我们这次比赛尝试过的数据增强手段，并且根据测试得分给出一些评价：

- ImageNet 图像预处理

  ```
  mean = [0.485,  0.456, 0.406]
  std = [0.229, 0.224, 0.225]
  ```

  比赛期间我们是将图片标准化后进行噪声攻击然后在标准化空间限制噪声大小，最后转换为原图的噪声，赛后我看到有队伍在 model 前加入一个标准化的线性层，效果是一样的

- rescaling and padding：非常有效的手段，是现在这类比赛必备的变换

- rotate：我们在数据处理的多个位置测试过这类变换，也考虑了多个角度的旋转，没有明显的提升

- adding Gaussian noise：在对图片攻击前加入随机的高斯噪声，效果不是很好

- translation of image：图片像素平移，效果不错；我们采用了清华

- jpeg compression：在对图片攻击做jpeg压缩，有些效果

- feature squeezing：并没测试，效果未知

- image augumentation：公开的baseline使用了这个变换，但是我们没使用

- flip：翻转图片，效果没提升

- scaler change（lightening\darkening）：将图片像素值分别除以2的m次方，提升明显

- gaussian filter：结束做高斯模糊，将噪声转换为黑白的；这个效果非常明显，赛后我看到2019年的新工作才知道在有防御的场景下低频噪声的效果要好于高频噪声

- 离散余弦变换滤波：类似上面的高斯滤波，比赛期间没采用

在这部分推荐几个比较好用的库：

1. 一个针对 numpy image 处理的库 scipy.ndimage
2. [Image augmentation for machine learning experiments —— imgaug](https://github.com/aleju/imgaug)
3. [https://github.com/mdbloice/Augmentor —— Augmentor](https://github.com/mdbloice/Augmentor)

### 模型融合

模型融合能够防止过拟合，并且在图片对抗攻击中能够显著的提升效果。这类技巧在很多比赛都可以使用，这里不再赘述。

### 初始化噪声选择

这个主要是想初始化一种噪声，然后引导模型利用梯度去改变噪声达到更好的攻击效果。我们在这次比赛使用了下面几种初始化的噪声：

- perlin noise
- 目标图片
- 一些论文生成的噪声

在防御模型下攻击效果不是很好。。

### 损失函数改进

常用的损失函数就是交叉熵，在比赛期间一种有效的改进方案是将针对无目标攻击的和有目标攻击的 loss 函数使用权重融合在一起。

### Dropout

开启这个等价于增强模型，也是非常有效的上分手段。

### 重要的论文梳理

将比赛参考过得论文按照我的分类做了一个清单，以供以后参考；

#### 黑盒攻击论文

- cross domain transferability of adversarial perturbations
- Adversarial Transformation Networks Learning to Generate Adversarial Examples
- AT-GAN A Generative Attack Model for Adversarial Transferring on Generative Adversarial Net
- black_box_adversarial_attack_with_transferable_model_based_embedding
- BYPASSING FEATURE SQUEEZING BY INCREASING ADVERSARY STRENGTH
- DELVING INTO TRANSFERABLE ADVERSARIAL EXAMPLES AND BLACK-BOX ATTACKS
- Dong_Efficient_Decision-Based_Black-Box_Adversarial_Attacks_on_Face_Recognition_CVPR_2019_paper
- EOT - Synthesizing Robust Adversarial Examples
- Generative Adversarial Perturbations
- Huang_Enhancing_Adversarial_Example_Transferability_With_an_Intermediate_Level_Attack_ICCV_2019_paper
- NATTACK Learning the Distributions of Adversarial Examples for an Improved Black-Box Attack on Deep Neural Networks
- Practical Black-Box Attacks against Machine Learning
- understanding_adversarial_examples
- Universal adversarial perturbations
- Procedural Noise Adversarial Examples for Black-Box Aacks on Deep Convolutional Networks
- On the Effectiveness of Low Frequency Perturbations

#### 防御论文

- Raff_Barrage_of_Random_Transforms_for_Adversarially_Robust_Defense_CVPR_2019_paper
- ADVERSARIAL MACHINE LEARNING AT SCALE
- ENSEMBLE ADVERSARIAL TRAINING ATTACKS AND DEFENSES
- ADVERSARIAL TRAINING EMBEDDING ADVERSARIAL PERTURBATIONS INTO THE PARAMETER SPACE OF A NEURAL NETWORK TO BUILD A ROBUST SYSTEM
- Defense against Adversarial Attacks Using High-Level Representation Guided Denoiser
- Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks
- Feature Denoising for Improving Adversarial Robustness
- Improving Black-box Adversarial Attacks with a Transfer-based Prior
- Improving the Generalization of Adversarial Training with Domain Adaptation
- Max-Mahalanobis Linear Discriminant Analysis Networks
- MemGuard Defending against Black-Box Membership Inference Attacks via Adversarial Examples
- Mitigating Adversarial Effects Through Randomization
- RazorNet Adversarial Training and Noise Training
- Transferable Adversarial Robustness using Adversarially Trained Autoencoders

#### FGSM家族白盒攻击

- Improving Transferability of Adversarial Examples with Input Diversity
- Boosting Adversarial Attacks with Momentum
- Improving Transferability of Adversarial Examples with Input Diversity
- NESTEROV ACCELERATED GRADIENT AND SCALE INVARIANCE FOR ADVERSARIAL ATTACKS
- Obfuscated Gradients Give a False Sense of Security

#### 查询黑盒攻击论文

这次比赛没有涉及，就不列出了

#### 清华出题人2019年新论文

- Efficient Decision-based Black-box Adversarial Attacks on Face Recognition
- Evading_Defenses_to_Transferable_Adversarial_Examples_by_Translation-Invariant_Attacks_CVPR_2019_paper
- Improving Adversarial Robustness via Promoting Ensemble Diversity
- MIXUP INFERENCE BETTER EXPLOITING MIXUP TO DEFEND ADVERSARIAL ATTACKS
- Rethinking Softmax Cross-Entropy Loss for Adversarial Robustness 
- Towards Robust Detection of Adversarial Examples

#### cihangxie 的论文

- 2019 Regional Homogeneity Towards Learning Transferable Universal Adversarial Perturbations Against Defenses
- Adversarial Attacks and Defences Competition
- Feature Denoising for Improving Adversarial Robustness
- Improving Transferability of Adversarial Examples with Input Diversity
- Learning Transferable Adversarial Examples via Ghost Networks
- MITIGATING ADVERSARIAL EFFECTS THROUGH RANDOMIZATION



